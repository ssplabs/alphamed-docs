# 基准联邦学习算法

AlphaMed 平台提供了一批常用联邦学习算法的实现，这些实现既可以用于实际的生产服务，也可以用于学习研究。

例如：当你想要借助 FedProx 算法完成一个联邦学习任务时，不需要去下载论文仔细研读，然后一行一行的完成代码的编写调试，再花时间进行测试验证。而是可以直接使用平台内置的 FedProx 基准算法实现，配合你选定的模型与本地数据，执行联邦学习任务即可。如果基准算法的默认实现在某些方面无法满足你的需求，也可以通过继承的方式在对应的地方修改既有实现即可，显著的减轻了开发工作量。

再比如：你实现了一个自己的算法，想要验证一下新算法的效果，和其它算法做一些对比。此时可以选择几个基准算法，和新算法一起，使用相同的模型和数据，分别运行联邦学习任务，将各自的评估指标放在一起进行对比，从而验证新算法的有效性。

## FedAvg 算法家族

FedAvg 算法是联邦学习中最常被引用和使用的算法，其基本原理是将参与方的本地模型参数上传到中央服务器，进行平均化后再下载到各个参与方进行更新。具体来说，FedAvg 算法包括以下步骤：

1. 随机选择参与方：中央服务器随机选择一部分参与方（例如10%）参与训练。
2. 发送模型参数：中央服务器将当前全局模型参数发送到参与方，并让他们使用本地数据进行训练。
3. 模型更新：参与方使用本地数据训练模型，并更新本地模型参数。
4. 参数聚合：中央服务器将参与方上传的本地模型参数进行加权平均，得到新的全局模型参数。
5. 参数分发：中央服务器将新的全局模型参数发送给所有参与方，用于他们下一轮的训练。

这样不断迭代，每一轮中央服务器随机选择一部分参与方进行训练，将局部模型进行聚合后再分发，从而达到联合学习的目的。由于该算法主要对模型参数进行传输和更新，因此可以保障数据隐私性和数据保密性。

AlphaMed 平台当前共有 4 个内置 FedAvg 家族基准算法：
- `FedAvgScheduler` 对应 MiniBatch SGD 版本的 FedAvg 算法，也是不考虑加密情况下使用最普遍的版本。
- `FedSGDScheduler` 对应 WholeBatch SGD 版本的 FedAvg 算法，训练速度非常慢，效果也一般，一般仅用于研究。`FedAvgScheduler` 的子类。
- `DPFedAvgScheduler` 使用差分隐私保护本地模型参数更新的 FedAvg 算法，训练速度与 MiniBatch SGD 版本的 FedAvg 算法相当，但会有一定的精度损失，且需要设置合适的超参数值。`FedAvgScheduler` 的子类。
- `SecureFedAvgScheduler` 使用安全聚合算法保护本地模型参数更新的 FedAvg 算法，训练速度较慢，优点是不会损失精度。`FedAvgScheduler` 的子类。

有关 FedAvg 算法使用的详细介绍，请参考[这里](../fed_avg/README.md)的介绍。

## FedProx 算法

FedProx 算法也是联邦学习场景中出镜率最高的算法之一。在联邦学习中，每个本地设备使用本地数据进行训练，并将更新的模型参数上传到服务器上。然而，由于本地设备之间存在巨大的数据分布不均和敏感性问题，因此上传的参数可能会引起安全和隐私问题。为了解决这一问题，FedProx 算法在损失函数中引入了惩罚项，它的作用是惩罚那些距离全局参数过远的本地参数，以实现联邦平均的稳定性和准确性。

有关 FedProx 算法使用的详细介绍，请参考[这里](../fed_prox/README.md)的介绍。
